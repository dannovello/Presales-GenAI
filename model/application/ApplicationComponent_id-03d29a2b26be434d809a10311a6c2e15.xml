<archimate:ApplicationComponent
    xmlns:archimate="http://www.archimatetool.com/archimate"
    name="multiple-prompt single conversation"
    id="id-03d29a2b26be434d809a10311a6c2e15"
    documentation="Generated knowledge prompting - first generates a list of most relevant facts related to a problem to be solved, then - after receiving the knowledge - prompts again with a question based on the received knowledge. The completion quality is usually higher, as the model can be conditioned on relevant facts.[1]&#xD;&#xA;Self-refine prompting[2] - first generates an initial solution, then - after receiving it - prompts again to generate its critique, then - after receiving the critique - prompts to address its own critique by generating a refined solution. A self-refining conversation stops when it runs out of tokens, time, or with a &quot;stop&quot; token.&#xD;&#xA;Maieutic (Socratic) prompting - first generates an explanation of the phenomena related to a problem to be solved, then - after receiving the explanation - prompts again to generate additional explanation for the least understood parts of explanation. Inconsistent explanation paths (trees) are pruned or discarded, improving complex commonsense reasoning.[3]&#xD;&#xA;Tree-of-thought prompting - first generates &quot;possible next steps&quot; for solving a problem, then - after receiving them - prompts again to run each of the proposed possible steps, using a breadth-first, beam, or some other method of tree search.[4][5]&#xD;&#xA;Least-to-most prompting - first generates the sub-problems to a problem to be solved, then - after receiving the sub-problems - prompts again to solve those in sequence, such that later sub-problems can be solved with the help of answers to previous sub-problems.[6]&#xD;&#xA;&#xD;&#xA;[1] Liu, Jiacheng; Liu, Alisa; Lu, Ximing; Welleck, Sean; West, Peter; Le Bras, Ronan; Choi, Yejin; Hajishirzi, Hannaneh (May 2022). &quot;Generated Knowledge Prompting for Commonsense Reasoning&quot;. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Dublin, Ireland: Association for Computational Linguistics: 3154–3169. doi:10.18653/v1/2022.acl-long.225. S2CID 239016123.&#xD;&#xA;[2]  Madaan, Aman; Tandon, Niket; Gupta, Prakhar; Hallinan, Skyler; Gao, Luyu; Wiegreffe, Sarah; Alon, Uri; Dziri, Nouha; Prabhumoye, Shrimai; Yang, Yiming; Gupta, Shashank; Prasad Majumder, Bodhisattwa; Hermann, Katherine; Welleck, Sean; Yazdanbakhsh, Amir (2023-03-01). &quot;Self-Refine: Iterative Refinement with Self-Feedback&quot;. arXiv:2303.17651.&#xD;&#xA;[3]  Jung, Jaehun; Qin, Lianhui; Welleck, Sean; Brahman, Faeze; Bhagavatula, Chandra; Le Bras, Ronan; Choi, Yejin (2022-05-01). &quot;Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations&quot;. arXiv:2205.11822.&#xD;&#xA;[4]  Long, Jieyi (2023-05-15). &quot;Large Language Model Guided Tree-of-Thought&quot;. arXiv:2305.08291 [cs.AI].&#xD;&#xA;[5]  Yao, Shunyu; Yu, Dian; Zhao, Jeffrey; Shafran, Izhak; Griffiths, Thomas L.; Cao, Yuan; Narasimhan, Karthik (2023-05-17). &quot;Tree of Thoughts: Deliberate Problem Solving with Large Language Models&quot;. arXiv:2305.10601 [cs.CL].&#xD;&#xA;[6]  Zhou, Denny; Schärli, Nathanael; Hou, Le; Wei, Jason; Scales, Nathan; Wang, Xuezhi; Schuurmans, Dale; Cui, Claire; Bousquet, Olivier; Le, Quoc; Chi, Ed (2022-05-01). &quot;Least-to-Most Prompting Enables Complex Reasoning in Large Language Models&quot;. arXiv:2205.10625."/>
